p8106\_hw1\_qw2331
================

### Data Overview

``` r
# Import data
trainHousing <- read_csv("./data/housing_training.csv")
testHousing <- read_csv("./data/housing_test.csv")

# For each column in train data
# Check missing values
sum(colSums(is.na(trainHousing)))
```

-   In general, there are `25` predictors and `1440` observations
    without missing values.  
-   The response of the data is `Sale_Price`.  
-   There are `4` categorical predictors so additional transformations
    are needed as they are difficult to interpret in the result and will
    potentially cause other problems.

### Data Preprocessing

``` r
# Convert categorical into dummy variables
train_x <- model.matrix(Sale_Price ~ ., trainHousing)[ ,-1]
train_y <- trainHousing$Sale_Price
test_x <- model.matrix(Sale_Price ~ ., testHousing)[ ,-1]
test_y <- testHousing$Sale_Price

# Correlation plot
corrplot(cor(train_x), 
         type = "upper", tl.cex = .8, tl.col = "black")
```

<img src="p8106_hw1_qw2331_files/figure-gfm/unnamed-chunk-2-1.png" width="90%" />

### Least Squares

``` r
set.seed(1234)

lm_fit <- train(train_x, train_y,
                method = "lm",
                trControl = trainControl(method = "cv", number = 10))

# Show coefficients of 38 predictors
lm_fit$finalModel$coefficients[-1]
```

    ##                Gr_Liv_Area               First_Flr_SF 
    ##                  24.580013                  42.524807 
    ##              Second_Flr_SF              Total_Bsmt_SF 
    ##                  41.767625                  35.191950 
    ##            Low_Qual_Fin_SF               Wood_Deck_SF 
    ##                         NA                  12.024295 
    ##              Open_Porch_SF                Bsmt_Unf_SF 
    ##                  16.177799                 -20.869718 
    ##               Mas_Vnr_Area                Garage_Cars 
    ##                  10.458006                4228.629576 
    ##                Garage_Area                 Year_Built 
    ##                   7.769075                 325.118788 
    ##              TotRms_AbvGrd                  Full_Bath 
    ##               -3838.100643               -4340.544104 
    ##        Overall_QualAverage  Overall_QualBelow_Average 
    ##               -5013.000215              -12799.206345 
    ##      Overall_QualExcellent           Overall_QualFair 
    ##               72606.089958              -11146.857536 
    ##           Overall_QualGood Overall_QualVery_Excellent 
    ##               12263.080342              130377.046730 
    ##      Overall_QualVery_Good           Kitchen_QualFair 
    ##               37975.180534              -26628.516431 
    ##           Kitchen_QualGood        Kitchen_QualTypical 
    ##              -18786.749812              -26765.115583 
    ##                 Fireplaces           Fireplace_QuFair 
    ##               11380.833066               -7206.509633 
    ##           Fireplace_QuGood   Fireplace_QuNo_Fireplace 
    ##                 606.991938                3394.266867 
    ##           Fireplace_QuPoor        Fireplace_QuTypical 
    ##               -5184.596707               -6397.856038 
    ##             Exter_QualFair             Exter_QualGood 
    ##              -38540.876203              -19936.203490 
    ##          Exter_QualTypical               Lot_Frontage 
    ##              -24361.041343                 102.402170 
    ##                   Lot_Area                  Longitude 
    ##                   0.604167              -34813.221408 
    ##                   Latitude                   Misc_Val 
    ##               58744.294511                   0.917129 
    ##                  Year_Sold 
    ##                -645.465513

**Disadvantage:** Based on both the correlation plot and the modeling
result, the disadvantage of using least squares to fit a linear model in
this case is that there are some collinear covariates. When two
predictor variables are highly correlated, the variance of the estimated
function will increase and lead to a higher MSE and lower prediction
accuracy.

### Lasso

``` r
set.seed(1234)

cv_lasso <- cv.glmnet(train_x, train_y,
                      alpha = 1,
                      lambda = exp(seq(11, 3, length = 100)))

par(mfrow = c(1, 2))
plot(cv_lasso)
plot_glmnet(cv_lasso$glmnet.fit)
```

<img src="p8106_hw1_qw2331_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />

``` r
# Number of predictors when using lambda.1se
predict(cv_lasso, s = "lambda.1se", type = "coefficients")
```

    ## 40 x 1 sparse Matrix of class "dgCMatrix"
    ##                               lambda.1se
    ## (Intercept)                -2.224119e+06
    ## Gr_Liv_Area                 5.659862e+01
    ## First_Flr_SF                1.126315e+00
    ## Second_Flr_SF               .           
    ## Total_Bsmt_SF               3.675070e+01
    ## Low_Qual_Fin_SF            -2.606145e+01
    ## Wood_Deck_SF                8.305179e+00
    ## Open_Porch_SF               7.878145e+00
    ## Bsmt_Unf_SF                -1.945568e+01
    ## Mas_Vnr_Area                1.425948e+01
    ## Garage_Cars                 3.071512e+03
    ## Garage_Area                 1.134094e+01
    ## Year_Built                  3.138267e+02
    ## TotRms_AbvGrd              -1.223942e+03
    ## Full_Bath                   .           
    ## Overall_QualAverage        -3.060241e+03
    ## Overall_QualBelow_Average  -9.072751e+03
    ## Overall_QualExcellent       9.023669e+04
    ## Overall_QualFair           -6.225791e+03
    ## Overall_QualGood            9.829696e+03
    ## Overall_QualVery_Excellent  1.601167e+05
    ## Overall_QualVery_Good       3.611322e+04
    ## Kitchen_QualFair           -5.186951e+03
    ## Kitchen_QualGood            .           
    ## Kitchen_QualTypical        -9.676461e+03
    ## Fireplaces                  6.462742e+03
    ## Fireplace_QuFair            .           
    ## Fireplace_QuGood            4.727746e+03
    ## Fireplace_QuNo_Fireplace    .           
    ## Fireplace_QuPoor            .           
    ## Fireplace_QuTypical         .           
    ## Exter_QualFair             -1.443030e+04
    ## Exter_QualGood              .           
    ## Exter_QualTypical          -5.099589e+03
    ## Lot_Frontage                7.002012e+01
    ## Lot_Area                    5.587230e-01
    ## Longitude                  -1.029452e+04
    ## Latitude                    1.634862e+04
    ## Misc_Val                    .           
    ## Year_Sold                   .

``` r
# Calculate the test error
lasso_rmse <- 
  RMSE(pred = predict(cv_lasso, newx = test_x, 
                      s = "lambda.min", type = "response"), obs = test_y)
```

**Results:** The test error of the lasso regression model is
`2.1032646\times 10^{4}`. And when the 1SE rule is applied, there are
`29` predictors included in the model.

### Elastic net

``` r
set.seed(1234)

enet_fit <- train(train_x, train_y,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 20),
                                         lambda = exp(seq(8, 0, length = 100))),
                  trControl = trainControl(method = "cv", number = 10))

plot(enet_fit, xTrans = log)
```

<img src="p8106_hw1_qw2331_files/figure-gfm/unnamed-chunk-5-1.png" width="90%" />

``` r
enet_fit$bestTune
```

    ##          alpha   lambda
    ## 183 0.05263158 754.6565
